---
title: "GKEクラスターの作成"
excerpt: ""
permalink: /docs/ja/8.1.1/
redirect_from:
  - /theme-setup/
toc: true
toc_sticky: false
sidebar:
  nav: "ja"
---

## GKEクラスターを作成する方法を説明します。

### Google Kubernetes Engine(GKE)にログイン

<https://cloud.google.com/kubernetes-engine/> で Google cloud にログインします。

### サービスアカウントの作成

### 1. IAMおよび管理者のサービスアカウントを作成します。

* サービスアカウントには、次の役割が必要です。

    * project/viewer

    * kubernetes-engine/admin

    * service-account/user

    * 役割ベースのアクセス制御(Identity and Access Management)を使用するための必要条件：  
    **RBAC生成時の前提条件の要素が必要**.

        * 次のコマンドを実行して、Kubernetesで役割を作成する権限をユーザに付与する必要があります。 [USER_ACCOUNT]は、ユーザーの電子メールアドレスです。  
        参照: <https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control>

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin --user [USER_ACCOUNT]
        ```

        or

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin \
        --user $(gcloud config get-value account)
        ```


* サービスアカウントの作成を選択します。  
**参照:** [サービスアカウントでCloud Platform認証](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

    * サービスアカウントの詳細 

        * **サービスアカウント名:**  
        このサービスアカウントの表示名です.

        * **作成**を選択します。

    * サービスアカウントの権限（オプション）

        * **役割:**  
        この使用説明書では、便宜上、所有者権限を付与します。

    * Private key 作成

        * **CREATE KEY** 選択します。

            * **CREATE** します。  
            サービスアカウントが作成されたら、サービスアカウントのユーザー認証情報が含まれているJSONのキーファイルがコンピュータにダウンロードされます。このキーファイルは、ユーザーのAPIの認証を実行するようにアプリケーションを構成するために使用されます。

    ![gke-create-service-account-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-1.png)
    ![gke-create-service-account-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-2.png)
    ![gke-create-service-account-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-service-account-3.png)


### GKEクラスターの作成
**参照:** [クラスターアーキテクチャ](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture)

### 1. プロジェクトの作成
**参照:** <https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster>

* プロジェクトの作成を選択します。[詳細](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

    * **Project Name:**  
    プロジェクトの名前を入力します。

        * **Preject ID**  
        プロジェクトIDは、プロジェクトのグローバルで一意の識別子です。プロジェクトを作成した後は、プロジェクトIDを変更することができません。  
        プロジェクトを作成するとき、またはプロジェクトIDを作成するためのAPIを有効にすると、選択したカスタム設定された名前になります。削除されたプロジェクトのプロジェクトIDは再利用することはできません。

    * **Location:**  
    親組織またはフォルダを選択します。

    ![gke-create-project-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-project-1-1.png)


### 1.1 Google Kubernetes Engine APIが有効かどうかを確認します。

    **GOOGLE KUBERNETES ENGINEAPI 利用設定** を確認します。

    ![gke-create-api-lib-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-api-lib-1.png)

* [Cloud SDK](https://cloud.google.com/sdk/downloads?hl=ko)がインストールされていることを確認します。

    * gcloud コマンドラインツールのデフォルト値を設定します。  
    gcloud コマンドラインツールで **プロジェクトID** と **Compute Engine 領域** オプションを入力する時間を節約するためにデフォルト値として設定することができます。

        ```
        gcloud config set project [PROJECT_ID]
        gcloud config set compute/zone us-central1-b
        ```

    * gcloudを最新バージョンにアップデートします。

        ```
        gcloud components update
        ```

### 2. VPCネットワーク作成

* VPC 네트워크 만들기 선택

    * **名前:**  
    * VPCネットワークの作成を選択 

    * **サブネットの作成**:  
    サブネットを使用すると、Google Cloud内自体プライベートクラウドトポロジを作成することができます。各地域にサブネットを作成するには、「自動」をクリックして、サブネットを直接定義するには、「カスタマイズ」をクリックします。 [詳細](https://cloud.google.com/compute/docs/subnetworks)

        * **名前:**  
            サブネット識別名を付与します。

        * **Region:**  
            ここでは、asia-northeast1（Tokyo）を使用します。  
            [詳細](https://cloud.google.com/compute/docs/regions-zones/regions-zones)  

        * **IPアドレスの範囲:**  
            CIDR表記で表したが、サブネットのアドレス範囲です。標準プライベートVPCネットワークアドレスの範囲（例えば、10.0.0.0/9）を使用して下さい。  
            [詳細](https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips)
        
        * **完了**を選択します。

        * 上記のような方法でsubnetを追加します。

    * **作成*を選択します。

    ![gke-create-vpc-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-1.png)
    ![gke-create-vpc-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-2.png)
    ![gke-create-vpc-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-vpc-3.png)

### 3. クラスターテンプレート
GCPコンソールを使用して、新しいクラスタを作成する場合は、使用可能なすべてのクラスタのテンプレートが表示されます。基本的には標準的なテンプレートが選択されます。

* 以下のテンプレートを使用することができます。

    * **標準クラスター:**  
    継続統合、ウェブを提供し、バックエンド用です。追加のカスタマイズが必要な場合や任意のテンプレートを選択するかわからない場合に選択すると、最も適しています。

    * **最初のクラスター:**  
    以下の強力なノードを実行して、自動拡張など、いくつかの高度な機能を使用していない小さなクラスタです。

    * **CPU集中アプリケーション:**  
    そのノードが標準のクラスタよりも強力なマルチコアCPUを提供するクラスタです。

    * **메모리 집중 애플리케이션:**  
    해당 노드가 일반적으로 강력한 다중 코어 CPU와 대용량 메모리를 제공하는 클러스터입니다.

    * **GPUアクセラレーションコンピューティング:**  
    基本ノードプールより強力なノードで構成されGPU設定ノードプールに追加で含まれているクラスタです。 [自動拡張](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler?hl=ko)は、基本的には使用されません。

    * **高可用性:**  
    クラスタが地域クラスターで構成され、特定の地域の各領域にクラスターマスターが提供されます。自動拡張とメンテナンス期間が使用されます。

* **Kubernetes クラスターを構成する**

    * **標準クラスター**  
    テンプレートを選択するか、ワークロードに適したテンプレートを選択します。

    * 必要に応じてテンプレートをカスタマイズします。次のフィールドは必須です。

        * **名前:**  
            クラスタ名を選択します。プロジェクトと領域内で一意である。

        * **場所の種類:**  
            クラスタ内のすべてのノードが同じ領域にあることを、指定した地域のすべての領域にあることができるかどうかです。

            * **エリア:**  
                場所の種類が領域である場合は、クラスタを作成するCompute Engine [コンピューティング領域](https://cloud.google.com/compute/docs/regions-zones/)です。  
                ここでは、 **asia-northeast1-a**(Tokyo)を使用します。


            * **지역:**  
                기본적으로 클러스터는 개발자가 생성 시 지정하는 단일 컴퓨팅 영역에 클러스터 마스터와 해당 노드를 만듭니다. 지역 클러스터를 만들어서 클러스터의 가용성 및 복구성을 향상시킬 수 있습니다.  
                [詳細](https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters)
        
        * **마스터 버전:**  
        kubernetes version을 선택합니다.

        * **노드 풀:**  
            노드 풀은 클러스터에서 Kubernetes를 실행하는 별도의 인스턴스 그룹입니다.

            * **노드 수:**  
                클러스터에 만들 노드 수입니다. 노드 및 리소스에 대해 사용 가능한 [리소스 할당량](https://cloud.google.com/compute/quotas)이 있어야 합니다(예: 방화벽 경로).

            * **머신 유형:**  
                인스턴스에 사용할 Compute Engine [머신 유형](https://cloud.google.com/compute/docs/machine-types)입니다. 각 머신 유형은 서로 다르게 청구됩니다. 기본 머신 유형은 n1-standard-1입니다. 머신 유형 가격 정보는 [머신 유형 가격표](https://cloud.google.com/compute/pricing)를 참조하세요.

            * 고급 수정:

                * **이름:**  
                노드 풀 이름을 부여합니다.

                * **부팅 디스크 크기(GB):**  
                이 사용 설명서에서는 10GB를 사용합니다.

                * 보안

                    * **서비스 계정:**  
                    **Compute Engine default service account 를 선택 합니다.**  
                    VM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 콘솔 메뉴의 권한을 사용하여 서비스 계정을 만들거나 기본 서비스 계정이 있으면 이 계정을 사용하세요. [詳細](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

                * **저장**을 선택합니다.
            
        * **고급옵션:**
            * 네트워킹

                * **VPC 네이티브:**  
                    VPC 네이티브 사용 설정(별칭 IP 사용) 체크합니다.

                * **네트워크:**  
                    항목에서 생성한 VPC를 선택합니다.

                * **노드 서브넷:**  
                    항목에서 생성한 subnet을 선택합니다.

    * **만들기**를 선택합니다.

    ![gke-create-cluster-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-1.png)
    ![gke-create-cluster-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-2.png)
    ![gke-create-cluster-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-3.png)
    ![gke-create-cluster-4]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-4.png)

    GCP 콘솔에서 클러스터를 만든 다음에는 해당 클러스터와 상호작용하도록 kubectl을 구성해야 합니다. 자세한 내용은 [kubeconfig 항목 생성](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)을 참조하세요.

### 4. 클러스터에 연결

* **Cloud Shell에서 실행:**  

    * Kubernetes 클러스터 목록에서 생성된 클러스터 **연결**을 선택합니다.

    ![gke-create-cluster-connect-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-1.png)

* **kubectl 을 사용하려면:**

    * kubectl 구성이 선행되야 합니다.  
    kubectl은 Kubernetes Engine에서 사용되는 클러스터 조정 시스템인 Kubernetes를 관리하기 위해 사용됩니다.

        * gcloud를 사용하여 kubectl을 설치할 수 있습니다.  

            ```
            gcloud components install kubectl
            ```

    * 클러스터 연결 화면에서 명령줄을 복사 콘솔에서 실행합니다.

        ![gke-create-cluster-connect-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-1.png)

    * 아래 명령어로 확인할 수 있습니다.

        ```
        kubectl get svc
        ```
        ![gke-create-cluster-connect-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-cluster-connect-2.png)

### NFS Sever 만들기
Google Compute Engine 영구 디스크를 사용하여 네트워크 파일 시스템 (NFS) 서버를 만들어 컨테이너에 마운트하는 것입니다.

### 1. Create an instance

* **이름(Name):**  
인스턴스 이름을 부여합니다.

* **지역(Region):**  
지역은 리소스를 실행할 수 있는 특정 지리적 위치입니다.

* **영역(Zone):**  
영역은 지역 내의 격리된 위치입니다. 영역은 사용할 수 있는 컴퓨팅 리소스와 데이터를 저장하고 사용할 위치를 결정합니다.

* **머신 유형(Machine type):**  
맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.

* **부팅 디스크(Boot disk):**  
각 인스턴스에는 부팅을 위한 디스크가 필요합니다. 이미지나 스냅샷을 선택하여 새 부팅 디스크를 생성하거나 기존 디스크를 인스턴스에 연결하세요.  
이 사용 설명서에서는 CentOS 7 사용합니다.

* **ID 및 API 액세스(Identity and API access):**  
생성한 서비스 계정을 선택합니다.  
VM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 사용할 서비스 계정과 허용할 API 액세스 수준을 선택하세요. [詳細](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances?hl)

    ![gke-create-instance-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-1.png)

 **Management, security, disks, networking, sole tenancy 확장 섹션**을 활성화 해서 아래 설정을 합니다.
 
* **디스크(Disks) 설정**

    * Add new disk를 선택 합니다.

        * 노드에 필요한 디스크 사양을 설정 합니다.

        ![gke-create-instance-disk-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-disk-1.png)

* **네트워크(Networking) 설정**  
네트워크는 인스턴스에서 액세스할 수 있는 네트워크 트래픽을 결정합니다.

    * **Network tags(네트워크 태그):**  
    네트워크 태그를 할당하여 특정 VM 인스턴스에 방화벽 규칙을 적용합니다.

    * **Network interfaces(네트워크 인터페이스)**

        * **Network(네트워크):**  
        목록에서 VPC network에서 생성한 VPC를 선택합니다.

        * **Subnetwork(하위 네트워크):**  
        목록에서 VPC network에서 생성한 subnet을 선택합니다.

        * **Primary internal IP(기본 내부 IP):**  
        임시의 경우 인스턴스를 다시 시작해도 내부 IP가 변경되지 않지만 인스턴스를 삭제하고 다시 만들면 내부 IP가 변경됩니다.  
        '임시(자동)'를 선택하여 하위 네트워크 범위의 주소를 할당하거나 '임시(커스텀)'를 선택하여 직접 입력하세요.  
        **인스턴스를 삭제하고 다시 만들 때 IP를 유지하려면 고정 내부 IP 주소를 선택하거나 만드세요.**  
        [詳細](https://cloud.google.com/compute/docs/subnetworks)

            ![gke-create-instance-network-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-network-2.png)

        * done(완료)**를 선택합니다.

* **Create(만들기)**를 선택합니다.

### 2. 생성된 인스턴스(노드) 방화벽 설정(SSH / NFS).

* 인스턴스 화면에서 생성한 인스턴스의 확장 메뉴에서 **View network details**를 선택.  
또는 VPC network > Firewall rules를 선택한다.

![gke-create-instance-view-network]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-instance-view-network.png)

* **SSH 방화벽 규칙을 추가 한다.**

    * **Name:**  
    예) cocktail-test-gke-demo-storage-ssh

    * **Network:**  
    인스턴스에서 설정한 VPC를 선택합니다.

    * **Targets(대상):**  
    **Specified target tags**를 선택.  
    가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.

        * **Target tags:**  
        NFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.

        * **Source filter:**  
        **IP ranges**를 선택.  
        필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.

        * **Source IP ranges:**  
        접근하고자 하는 IP ranges를 CIDR 표기법으로 추가 합니다.  
        예)10.0.10.0/24 192.168.100.5/32

        * **Protocols and ports:**  
        허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.

            * Specified protocols and ports

                * **tcp:** 22

    ![gke-create-firewall-rules-ssh]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-firewall-rules-ssh.png)

* **NFS 방화벽 규칙을 추가 한다.**

    * **Name:**  
    예) cocktail-test-gke-demo-storage-nfs

    * **Network:**  
    인스턴스에서 설정한 VPC를 선택합니다.

    * **Targets(대상):**  
    **Specified target tags**를 선택.  
    가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.

        * **Target tags:**  
        NFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.

        * **Source filter:**  
        **Subnets**를 선택.  
        필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.

        * **Subnets:**  
        이 소스 하위 네트워크의 트래픽만 허용됩니다.  

        * **Protocols and ports:**  
        허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.

            * Allow all 선택  
            이 사용 설명서에서는 편의상 Alloww all을 선택합니다.

    ![gke-create-firewall-rules-nfs]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-firewall-rules-nfs.png)

### 3. 생성된 인스턴스(노드)에 SSH 연결.

* 인스턴스 화면에서 생성된 인스턴스(노드)의 **SSH**항목을 선택 -> **View gcloud command**를 선택 한다(자동생성).

* **Run IN CLOUD SHELL**을 실행 한다.

    * RSA key pair 가 자동생성 된다.  
    아래명령으로 생성된 key pair 을 확인할 수 있다.

        ```
        cd ~/.ssh
        ```

    * RSA key pair GCP의 Compute Engine > Metadata > SSH Keys에 자동 등록 된다.  
    아래 그림처럼 등록된 SSH Key를 확인 및 편집할 수 있다.

![gke-create-connect-node-ssh-1]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-1.png)

* 또는 RSA key pair 수동 생성 / 등록 방법.

    * 아래 명령으로 RSA key pair를 생성한다.

        ```
        ssh-keygen -t rsa -f ./rsa-gcp-key -C"<Your-email.com>"
        ```
    
    ![gke-create-connect-node-ssh-2]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-2.png)

    * 생성된 RSA Key 내용을 복사 해서 Compute Engine > Metadata > SSH Keys 에 등록 한다.

        ```
        cat ./rsa-gcp-key.pub
        ```

        ![gke-create-connect-node-ssh-3]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-3.png)
        ![gke-create-connect-node-ssh-4]({{ site.baseurl }}/assets/KR/{{ site.version }}/GKE/gke-create-connect-node-ssh-4.png)


### 4. 생성된 인스턴스(노드)에 NFS 서버 설치 / 구성

* 노드 OS 확인.

    ```
    sudo grep . /etc/*-release
    ```

* 노드 NFS 패키지 확인.

    ```
    sudo rpm -qa | grep nfs
    ```

* 노드 NFS 패키지 설치.

    ```
    sudo yum install nfs-utils nfs-utils-lib
    ```

* 클러스터(마스터)에서 NFS에 사용될 공유 디렉토리를 생성 한다.  
**provisioner Deployment**에서 **NFS_PATH**에 지정된 디렉토리를 생성한다.

    ```
    sudo mkdir /storage/shared
    ```

* 공유 디렉토리 퍼미션 설정.

    ```
    sudo chmod -R 777 /storage
    ```

* 공유 디렉토리 NFS 권한을 부여하고, 동기화를 한다.  
인스턴스(노드)에서 설정한 **Subnet 또는 Ip ranges**를 부여한다.

    ```
    # sudo vi /etc/exports

    /storage/shared 10.0.10.0/24(rw,sync)
    ```

* NFS service를 재시작 한다.

    ```
    sudo service nfs restart
    ```

### 5. 클러스터(마스터)에 NFS-Client Provisioner 구성.  
**참조:** [Kubernetes NFS-Client Provisioner](https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client)

* **Setup authorization:**  
클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.

    * **kubectl apply -f 1-rbac.yaml**  
    Namespace, ServiceAccount, Setup authorization 적용

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
    name: cocktail-addon
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: nfs-client-provisioner
    namespace: cocktail-addon
    ---
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: nfs-client-provisioner-runner
    rules:
    - apiGroups: [""]
        resources: ["persistentvolumes"]
        verbs: ["get", "list", "watch", "create", "delete"]
    - apiGroups: [""]
        resources: ["persistentvolumeclaims"]
        verbs: ["get", "list", "watch", "update"]
    - apiGroups: ["storage.k8s.io"]
        resources: ["storageclasses"]
        verbs: ["get", "list", "watch"]
    - apiGroups: [""]
        resources: ["events"]
        verbs: ["list", "watch", "create", "update", "patch"]

    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: run-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: ClusterRole
    name: nfs-client-provisioner-runner
    apiGroup: rbac.authorization.k8s.io
    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    rules:
    - apiGroups: [""]
        resources: ["endpoints"]
        verbs: ["get", "list", "watch", "create", "update", "patch"]
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: Role
    name: leader-locking-nfs-client-provisioner
    apiGroup: rbac.authorization.k8s.io
    ```

* **Configure the NFS-Client provisioner**

    * **kubectl apply -f 2-shared-storage-provisioner-dp.yaml**  
    Deployment, 다음 프로비저 배포 파일을 편집하여 NFS 서버에 대한 연결 정보를 추가해야합니다.

    ```yaml
    kind: Deployment
    apiVersion: apps/v1
    metadata:
    name: shared-storage-provisioner
    namespace: cocktail-addon
    spec:
    replicas: 1
    selector:
        matchLabels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        app: shared-storage-provisioner
    strategy:
        type: Recreate
    template:
        metadata:
        labels:
            app: shared-storage-provisioner
            acornsoft.io/provisioner-type: NFSDYNAMIC
        spec:
        serviceAccount: nfs-client-provisioner
        containers:
            - name: shared-storage-provisioner
            image: quay.io/external_storage/nfs-client-provisioner:v2.0.1
            volumeMounts:
                - name: nfs-client-root
                mountPath: /persistentvolumes
            env:
                - name: PROVISIONER_NAME
                # YOUR PROVISIONER_NAME
                value: acornsoft.io/shared-storage-provisioner
                - name: NFS_SERVER
                # YOUR NFS SERVER HOSTNAME
                value: 10.0.10.8
                - name: NFS_PATH
                value: /storage/shared
            resources:
                limits:
                cpu: 100m
                memory: 128Mi
                requests:
                cpu: 100m
                memory: 128Mi
        tolerations:
        - key: node-role.kubernetes.io/master
            effect: NoSchedule
        volumes:
            - name: nfs-client-root
            nfs:
                # YOUR NFS SERVER HOSTNAME
                server: 10.0.10.8
                path: /storage/shared

    ```

* **Storage class 등록**  
참조: <https://kubernetes.io/docs/concepts/storage/storage-classes/>

    * **kubectl apply -f 3-single-sc.yaml**  
    single-storage : Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # single-storage : Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
    name: single-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: GCE
        acornsoft.io/type: SINGLE
        addonmanager.kubernetes.io/mode: EnsureExists
        kubernetes.io/cluster-service: "true"
    parameters:
    type: pd-standard
    provisioner: kubernetes.io/gce-pd
    reclaimPolicy: Delete
    ```

    * **kubectl apply -f 4-shared-sc.yaml**  
    shared-storage : NFS Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # shared-storage : NFS Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
    name: shared-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        acornsoft.io/total-capacity: "100"
        acornsoft.io/type: SHARED
    provisioner: acornsoft.io/shared-storage-provisioner
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    ```

### 6. 샘플 PVC 등록 / 확인.

* 아래 명령으로 샘플을 구성할 수 있다.

    * **kubectl apply -f 5-test-pod.yaml**  
    임시 파일 생성 pod를 생성 한다.

    ```yaml
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
    name: nfs-pvc
    spec:
    storageClassName: shared-storage
    accessModes:
    - ReadWriteMany
    resources:
        requests:
        storage: 1Mi
    ---
    kind: Pod
    apiVersion: v1
    metadata:
    name: test-pod
    spec:
    containers:
    - name: test-pod
        image: gcr.io/google_containers/busybox:1.24
        command:
        - "/bin/sh"
        args:
        - "-c"
        - "touch /mnt/SUCCESS && exit 0 || exit 1"
        volumeMounts:
        - name: nfs-test-pvc
            mountPath: "/mnt"
    restartPolicy: "Never"
    volumes:
        - name: nfs-test-pvc
        persistentVolumeClaim:
            claimName: nfs-pvc
    ```

    * **kubectl get pv**  
    생성된 PV를 확인 한다.

    * **NFS 노드**에 생성된 임시 파일을 확인할 수 있다.
